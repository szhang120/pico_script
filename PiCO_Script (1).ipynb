{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEJkyqw8IvPJ",
        "outputId": "31d824ac-e830-4e74-9974-dd31c9abe00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Peer-review-in-LLMs'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 87 (delta 10), reused 76 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (87/87), 3.08 MiB | 13.50 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "/content/Peer-review-in-LLMs\n"
          ]
        }
      ],
      "source": [
        "# 1. Clone PiCO Github Repo files onto Colab Session\n",
        "\n",
        "!git clone https://github.com/PKU-YuanGroup/Peer-review-in-LLMs.git\n",
        "%cd Peer-review-in-LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxWjNXZlIzkU",
        "outputId": "a092b8fb-4324-4128-bffe-68e48bc14831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.9.5)\n",
            "Collecting fastapi (from -r requirements.txt (line 2))\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx (from -r requirements.txt (line 3))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting nh3 (from -r requirements.txt (line 5))\n",
            "  Downloading nh3-0.2.17-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.25.2)\n",
            "Requirement already satisfied: prompt_toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.0.45)\n",
            "Collecting pydantic<2,>=1 (from -r requirements.txt (line 8))\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.31.0)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (13.7.1)\n",
            "Collecting shortuuid (from -r requirements.txt (line 11))\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tiktoken (from -r requirements.txt (line 12))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting uvicorn (from -r requirements.txt (line 13))\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting accelerate>=0.21 (from -r requirements.txt (line 14))\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting peft (from -r requirements.txt (line 15))\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.1.99)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (4.41.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.20.3)\n",
            "Collecting openai<1 (from -r requirements.txt (line 20))\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting anthropic>=0.3 (from -r requirements.txt (line 21))\n",
            "  Downloading anthropic-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting ray (from -r requirements.txt (line 22))\n",
            "  Downloading ray-2.24.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (1.11.4)\n",
            "Collecting jsonlines (from -r requirements.txt (line 25))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting vllm (from -r requirements.txt (line 26))\n",
            "  Downloading vllm-0.4.3-cp310-cp310-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting einops (from -r requirements.txt (line 27))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting markdown2[all] (from -r requirements.txt (line 4))\n",
            "  Downloading markdown2-2.4.13-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 1)) (4.0.3)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->-r requirements.txt (line 2))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r requirements.txt (line 2)) (4.12.1)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi->-r requirements.txt (line 2))\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r requirements.txt (line 2)) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi->-r requirements.txt (line 2))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->-r requirements.txt (line 2))\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting orjson>=3.2.1 (from fastapi->-r requirements.txt (line 2))\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->-r requirements.txt (line 2))\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->-r requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->-r requirements.txt (line 3)) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx->-r requirements.txt (line 3))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->-r requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->-r requirements.txt (line 3)) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->-r requirements.txt (line 3))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: pygments>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from markdown2[all]->-r requirements.txt (line 4)) (2.16.1)\n",
            "Collecting wavedrom (from markdown2[all]->-r requirements.txt (line 4))\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit>=3.0.0->-r requirements.txt (line 7)) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->-r requirements.txt (line 10)) (3.0.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r requirements.txt (line 12)) (2024.5.15)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->-r requirements.txt (line 14)) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->-r requirements.txt (line 14)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->-r requirements.txt (line 14)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->-r requirements.txt (line 14)) (0.23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->-r requirements.txt (line 14)) (0.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft->-r requirements.txt (line 15)) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 17)) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 17)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 17)) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 17)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 17)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->-r requirements.txt (line 18)) (0.19.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic>=0.3->-r requirements.txt (line 21)) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from anthropic>=0.3->-r requirements.txt (line 21))\n",
            "  Downloading jiter-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->-r requirements.txt (line 22)) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->-r requirements.txt (line 22)) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 23)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 23)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 23)) (2024.1)\n",
            "Requirement already satisfied: cmake>=3.21 in /usr/local/lib/python3.10/dist-packages (from vllm->-r requirements.txt (line 26)) (3.27.9)\n",
            "Collecting ninja (from vllm->-r requirements.txt (line 26))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm->-r requirements.txt (line 26)) (9.0.0)\n",
            "INFO: pip is looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting vllm (from -r requirements.txt (line 26))\n",
            "  Downloading vllm-0.4.2-cp310-cp310-manylinux1_x86_64.whl.metadata (9.1 kB)\n",
            "  Downloading vllm-0.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
            "  Downloading vllm-0.4.0.post1-cp310-cp310-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting torch (from -r requirements.txt (line 17))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting xformers==0.0.23.post1 (from vllm->-r requirements.txt (line 26))\n",
            "  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm (from -r requirements.txt (line 26))\n",
            "  Downloading vllm-0.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "  Downloading vllm-0.3.3-cp310-cp310-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "  Downloading vllm-0.3.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "  Downloading vllm-0.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "INFO: pip is still looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading vllm-0.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "  Downloading vllm-0.2.7-cp310-cp310-manylinux1_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pydantic<2,>=1 (from -r requirements.txt (line 8))\n",
            "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioprometheus[starlette] (from vllm->-r requirements.txt (line 26))\n",
            "  Downloading aioprometheus-23.12.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting triton==2.1.0 (from torch->-r requirements.txt (line 17))\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->-r requirements.txt (line 3)) (1.2.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 2))\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi->-r requirements.txt (line 2))\n",
            "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi->-r requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->-r requirements.txt (line 10)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 23)) (1.16.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 2))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 2))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 2))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 2))\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 2))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting quantile-python>=1.1 (from aioprometheus[starlette]->vllm->-r requirements.txt (line 26))\n",
            "  Downloading quantile-python-1.1.tar.gz (2.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->-r requirements.txt (line 22)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->-r requirements.txt (line 22)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->-r requirements.txt (line 22)) (0.18.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 17)) (1.3.0)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->-r requirements.txt (line 4))\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->-r requirements.txt (line 2))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nh3-0.2.17-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.1/777.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.28.0-py3-none-any.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.7/862.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.24.0-cp310-cp310-manylinux2014_x86_64.whl (65.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading vllm-0.2.7-cp310-cp310-manylinux1_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.4.13-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioprometheus-23.12.0-py3-none-any.whl (31 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Building wheels for collected packages: wavedrom, quantile-python\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30055 sha256=2fcca49b725b1057c16a3c371941a610b553f358175f3cad3834a5303e5b6eb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
            "  Building wheel for quantile-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for quantile-python: filename=quantile_python-1.1-py3-none-any.whl size=3443 sha256=c30dd2de41f8570ab81c25da0ebca34060e746cb005c8e2855a7714b4bd31e35\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/f4/0a/0e7d01548a005f9f3fa23101f071d248da052f2a9bf2fe11c6\n",
            "Successfully built wavedrom quantile-python\n",
            "Installing collected packages: quantile-python, ninja, nh3, websockets, uvloop, ujson, triton, svgwrite, shortuuid, shellingham, python-multipart, python-dotenv, pydantic, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markdown2, jsonlines, jiter, httptools, h11, einops, dnspython, wavedrom, watchfiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, email_validator, aioprometheus, typer, openai, nvidia-cusolver-cu12, httpx, torch, ray, fastapi-cli, anthropic, xformers, fastapi, accelerate, vllm, peft\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.3\n",
            "    Uninstalling pydantic-2.7.3:\n",
            "      Successfully uninstalled pydantic-2.7.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.1.2 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 aioprometheus-23.12.0 anthropic-0.28.0 dnspython-2.6.1 einops-0.8.0 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 jiter-0.4.1 jsonlines-4.0.0 markdown2-2.4.13 nh3-0.2.17 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-0.28.1 orjson-3.10.3 peft-0.11.1 pydantic-1.10.13 python-dotenv-1.0.1 python-multipart-0.0.9 quantile-python-1.1 ray-2.24.0 shellingham-1.5.4 shortuuid-1.0.13 starlette-0.37.2 svgwrite-1.4.3 tiktoken-0.7.0 torch-2.1.2 triton-2.1.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 vllm-0.2.7 watchfiles-0.22.0 wavedrom-2.0.3.post3 websockets-12.0 xformers-0.0.23.post1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 2. Install requirements.txt, especially for the '\"vllm\" thing.\n",
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZR-GPtCLEj5",
        "outputId": "42f19c90-69bc-4019-c3a2-f7f8ecc56f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "''' 3. Data 1:\n",
        "Upload your zipped mt_bench2, containing 'questions.jsonl' (adversarial) and 'questions3' (old)\n",
        "as set up before like in our local folder. Next we'll actually get 'questions.jsonl' where it has to go.\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/My Drive/question.jsonl' data/"
      ],
      "metadata": {
        "id": "h87NIJAeMf9M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL ANSWER GENERATION\n",
        "# FOR EACH CLOSED-SOURCE PROVIDER, SET API KEY SECRET.\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "7WMaUM9ka47k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StJzq1VfJNAU",
        "outputId": "03c2f32e-cb40-4ff3-9195-786be8a8d5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output to data/mt_bench/model_answer/gpt-3.5-turbo.jsonl\n",
            "100% 24/24 [01:41<00:00,  4.22s/it]\n"
          ]
        }
      ],
      "source": [
        "# Model Answer Generation (Proprietary)\n",
        "\n",
        "# List of models for answer generation\n",
        "closed_models = [\n",
        "    {'model_path': 'openai/gpt-3.5-turbo', 'model_id': 'gpt-3.5-turbo'}\n",
        "]\n",
        "\n",
        "# Loop through each model and generate responses\n",
        "for model in closed_models:\n",
        "    !python llm_judge/gen_model_answer.py \\\n",
        "        --model-path {model['model_path']} \\\n",
        "        --model-id {model['model_id']} \\\n",
        "        --bench-name mt_bench"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Answer Generation (Open)\n",
        "\n",
        "open_models = [\n",
        "    {'model_path': 'lmsys/vicuna-7b-v1.5', 'model_id': 'vicuna-7b-v1.5'},\n",
        "    {'model_path': 'microsoft/Phi-3-mini-4k-instruct', 'model_id': 'Phi-3-mini-4k-instruct'},\n",
        "    {'model_path': 'lmsys/fastchat-t5-3b-v1.0', 'model_id': 'fastchat-t5-3b-v1.0'},\n",
        "    {'model_path': 'THUDM/chatglm-6b', 'model_id': 'chatglm-6b'}\n",
        "]\n",
        "\n",
        "for model in open_models:\n",
        "    !python llm_judge/gen_model_answer.py \\\n",
        "        --model-path {model['model_path']} \\\n",
        "        --model-id {model['model_id']} \\\n",
        "        --bench-name mt_bench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNyYXeU1y_HS",
        "outputId": "034455e1-37de-467f-996c-d3dacb7778e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output to data/mt_bench/model_answer/vicuna-7b-v1.5.jsonl\n",
            "tokenizer_config.json: 100% 749/749 [00:00<00:00, 5.87MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 18.6MB/s]\n",
            "special_tokens_map.json: 100% 438/438 [00:00<00:00, 3.95MB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 6.14MB/s]\n",
            "pytorch_model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 106MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 10.5M/9.98G [00:00<03:35, 46.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 31.5M/9.98G [00:00<01:44, 95.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 73.4M/9.98G [00:00<00:51, 194MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 126M/9.98G [00:00<00:34, 285MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 178M/9.98G [00:00<00:29, 333MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 220M/9.98G [00:00<00:28, 340MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 262M/9.98G [00:00<00:27, 357MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 315M/9.98G [00:01<00:25, 384MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 367M/9.98G [00:01<00:24, 400MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 419M/9.98G [00:01<00:23, 412MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 472M/9.98G [00:01<00:22, 418MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 524M/9.98G [00:01<00:22, 423MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 577M/9.98G [00:01<00:22, 414MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 619M/9.98G [00:01<00:22, 409MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 661M/9.98G [00:01<00:22, 410MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 703M/9.98G [00:01<00:22, 411MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 755M/9.98G [00:02<00:21, 422MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 807M/9.98G [00:02<00:21, 424MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 860M/9.98G [00:02<00:21, 415MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 902M/9.98G [00:02<00:22, 411MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 944M/9.98G [00:02<00:21, 411MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 986M/9.98G [00:02<00:21, 412MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.03G/9.98G [00:02<00:21, 409MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.07G/9.98G [00:02<00:22, 397MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.11G/9.98G [00:02<00:22, 398MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.15G/9.98G [00:03<00:22, 397MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.20G/9.98G [00:03<00:22, 397MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.24G/9.98G [00:03<00:21, 398MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.28G/9.98G [00:03<00:21, 403MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.32G/9.98G [00:03<00:21, 407MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.36G/9.98G [00:03<00:21, 408MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.41G/9.98G [00:03<00:20, 411MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.46G/9.98G [00:03<00:20, 414MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.50G/9.98G [00:03<00:20, 409MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.54G/9.98G [00:04<00:20, 407MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.58G/9.98G [00:04<00:20, 406MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/9.98G [00:04<00:20, 408MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.67G/9.98G [00:04<00:20, 408MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.71G/9.98G [00:04<00:20, 409MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.76G/9.98G [00:04<00:19, 414MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.81G/9.98G [00:04<00:19, 425MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.87G/9.98G [00:04<00:19, 424MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.92G/9.98G [00:04<00:20, 397MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.96G/9.98G [00:05<00:20, 386MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.01G/9.98G [00:05<00:20, 397MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.06G/9.98G [00:05<00:20, 390MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.11G/9.98G [00:05<00:19, 400MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.16G/9.98G [00:05<00:18, 412MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.21G/9.98G [00:05<00:18, 424MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.26G/9.98G [00:05<00:17, 436MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.32G/9.98G [00:05<00:17, 441MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.37G/9.98G [00:06<00:17, 433MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.42G/9.98G [00:06<00:20, 368MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.46G/9.98G [00:06<00:22, 330MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.51G/9.98G [00:06<00:25, 298MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.54G/9.98G [00:06<00:25, 288MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.57G/9.98G [00:06<00:27, 270MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.60G/9.98G [00:06<00:28, 256MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.63G/9.98G [00:07<00:29, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.66G/9.98G [00:07<00:31, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.69G/9.98G [00:07<00:31, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.73G/9.98G [00:07<00:33, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.76G/9.98G [00:07<00:34, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.79G/9.98G [00:07<00:33, 212MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.82G/9.98G [00:07<00:31, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.86G/9.98G [00:08<00:27, 257MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.92G/9.98G [00:08<00:23, 305MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.97G/9.98G [00:08<00:20, 339MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.01G/9.98G [00:08<00:20, 336MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.05G/9.98G [00:08<00:21, 317MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.09G/9.98G [00:08<00:22, 310MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.12G/9.98G [00:08<00:22, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.17G/9.98G [00:09<00:21, 319MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.21G/9.98G [00:09<00:19, 341MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.25G/9.98G [00:09<00:18, 358MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.29G/9.98G [00:09<00:17, 372MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.33G/9.98G [00:09<00:17, 378MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.38G/9.98G [00:09<00:17, 368MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.42G/9.98G [00:09<00:19, 335MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.46G/9.98G [00:09<00:18, 349MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.50G/9.98G [00:09<00:17, 366MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.54G/9.98G [00:10<00:16, 379MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.59G/9.98G [00:10<00:39, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.62G/9.98G [00:11<00:50, 126MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.67G/9.98G [00:11<00:36, 171MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.72G/9.98G [00:11<00:34, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.75G/9.98G [00:11<00:31, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.80G/9.98G [00:11<00:26, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.84G/9.98G [00:11<00:23, 267MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.88G/9.98G [00:11<00:20, 296MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.92G/9.98G [00:12<00:38, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.95G/9.98G [00:12<00:55, 109MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.97G/9.98G [00:13<01:04, 93.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.00G/9.98G [00:13<01:06, 90.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.02G/9.98G [00:13<01:16, 78.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.04G/9.98G [00:14<01:09, 85.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.07G/9.98G [00:14<01:01, 95.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.09G/9.98G [00:14<01:08, 86.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.10G/9.98G [00:14<01:11, 81.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.11G/9.98G [00:15<01:17, 75.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.15G/9.98G [00:15<00:53, 110MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.17G/9.98G [00:15<01:02, 93.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.18G/9.98G [00:15<01:05, 89.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.19G/9.98G [00:15<01:09, 82.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.22G/9.98G [00:16<01:04, 89.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.23G/9.98G [00:16<01:08, 84.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.24G/9.98G [00:16<01:18, 73.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.25G/9.98G [00:16<01:19, 71.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.26G/9.98G [00:16<01:26, 66.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.28G/9.98G [00:16<01:03, 89.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/9.98G [00:17<00:50, 113MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.32G/9.98G [00:17<00:54, 103MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.34G/9.98G [00:17<00:48, 117MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.37G/9.98G [00:17<00:43, 128MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.39G/9.98G [00:18<01:03, 87.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.41G/9.98G [00:18<01:08, 81.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.42G/9.98G [00:18<01:13, 75.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.47G/9.98G [00:18<00:44, 124MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.51G/9.98G [00:18<00:32, 168MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.55G/9.98G [00:18<00:25, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.58G/9.98G [00:19<00:23, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.63G/9.98G [00:19<00:18, 281MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.68G/9.98G [00:19<00:17, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.73G/9.98G [00:19<00:15, 337MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.77G/9.98G [00:19<00:14, 354MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.81G/9.98G [00:19<00:14, 365MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.85G/9.98G [00:19<00:13, 373MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.90G/9.98G [00:19<00:13, 365MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.95G/9.98G [00:19<00:13, 386MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.99G/9.98G [00:20<00:12, 389MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.03G/9.98G [00:20<00:13, 362MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.08G/9.98G [00:20<00:14, 338MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.12G/9.98G [00:20<00:14, 325MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.16G/9.98G [00:20<00:15, 316MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.20G/9.98G [00:20<00:15, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.23G/9.98G [00:20<00:15, 306MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.26G/9.98G [00:20<00:15, 301MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.30G/9.98G [00:21<00:15, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.33G/9.98G [00:21<00:15, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.36G/9.98G [00:21<00:15, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.39G/9.98G [00:21<00:15, 294MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.42G/9.98G [00:21<00:15, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.45G/9.98G [00:21<00:15, 291MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.48G/9.98G [00:21<00:15, 290MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.52G/9.98G [00:21<00:15, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.55G/9.98G [00:21<00:15, 290MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.58G/9.98G [00:22<00:15, 287MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.61G/9.98G [00:22<00:14, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.64G/9.98G [00:22<00:14, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.67G/9.98G [00:22<00:14, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.70G/9.98G [00:22<00:14, 301MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.74G/9.98G [00:22<00:13, 304MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.77G/9.98G [00:22<00:14, 301MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.80G/9.98G [00:22<00:13, 303MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.83G/9.98G [00:22<00:16, 244MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.86G/9.98G [00:23<00:15, 261MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.89G/9.98G [00:23<00:14, 275MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.93G/9.98G [00:23<00:14, 288MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.97G/9.98G [00:23<00:13, 294MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.00G/9.98G [00:23<00:13, 300MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.04G/9.98G [00:23<00:12, 304MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.08G/9.98G [00:23<00:12, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.12G/9.98G [00:23<00:12, 310MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.17G/9.98G [00:24<00:12, 312MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.20G/9.98G [00:24<00:12, 312MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.24G/9.98G [00:24<00:11, 314MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.27G/9.98G [00:24<00:11, 313MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.98G [00:24<00:11, 314MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.34G/9.98G [00:24<00:11, 313MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.38G/9.98G [00:24<00:11, 312MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.41G/9.98G [00:24<00:11, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.45G/9.98G [00:24<00:11, 313MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.48G/9.98G [00:25<00:11, 314MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.51G/9.98G [00:25<00:11, 313MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.54G/9.98G [00:25<00:11, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.57G/9.98G [00:25<00:11, 303MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.61G/9.98G [00:25<00:11, 296MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.64G/9.98G [00:25<00:11, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.67G/9.98G [00:25<00:11, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.70G/9.98G [00:25<00:11, 296MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.74G/9.98G [00:25<00:10, 303MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.77G/9.98G [00:26<00:16, 198MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.82G/9.98G [00:26<00:13, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.86G/9.98G [00:26<00:12, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.90G/9.98G [00:26<00:11, 271MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.94G/9.98G [00:26<00:10, 284MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.98G/9.98G [00:26<00:10, 293MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.01G/9.98G [00:26<00:09, 298MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.06G/9.98G [00:27<00:09, 304MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.10G/9.98G [00:27<00:09, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.13G/9.98G [00:27<00:09, 309MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.17G/9.98G [00:27<00:09, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.21G/9.98G [00:27<00:08, 313MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.26G/9.98G [00:27<00:08, 312MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.29G/9.98G [00:27<00:08, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.32G/9.98G [00:27<00:08, 310MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.35G/9.98G [00:28<00:08, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.38G/9.98G [00:28<00:08, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.41G/9.98G [00:28<00:08, 307MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.46G/9.98G [00:28<00:08, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.49G/9.98G [00:28<00:08, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.53G/9.98G [00:28<00:08, 279MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.56G/9.98G [00:29<00:14, 165MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.59G/9.98G [00:29<00:13, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.62G/9.98G [00:29<00:20, 113MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.64G/9.98G [00:29<00:19, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.67G/9.98G [00:30<00:18, 126MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.69G/9.98G [00:30<00:25, 91.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.71G/9.98G [00:30<00:27, 81.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.73G/9.98G [00:30<00:25, 88.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.75G/9.98G [00:31<00:27, 80.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.76G/9.98G [00:31<00:28, 76.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.77G/9.98G [00:31<00:29, 74.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.78G/9.98G [00:31<00:32, 68.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.79G/9.98G [00:31<00:32, 68.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.80G/9.98G [00:32<00:32, 66.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.81G/9.98G [00:32<00:31, 69.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.84G/9.98G [00:32<00:21, 100MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.85G/9.98G [00:32<00:22, 92.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.86G/9.98G [00:32<00:26, 81.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.89G/9.98G [00:33<00:25, 82.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.90G/9.98G [00:33<00:26, 77.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.91G/9.98G [00:33<00:27, 75.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.92G/9.98G [00:33<00:28, 72.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.93G/9.98G [00:33<00:29, 70.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.94G/9.98G [00:33<00:31, 64.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.96G/9.98G [00:34<00:28, 70.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.97G/9.98G [00:34<00:29, 69.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.98G/9.98G [00:34<00:29, 67.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.99G/9.98G [00:34<00:29, 67.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.01G/9.98G [00:34<00:22, 86.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.02G/9.98G [00:34<00:24, 79.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.03G/9.98G [00:35<00:26, 74.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.04G/9.98G [00:35<00:27, 70.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.07G/9.98G [00:35<00:18, 104MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.11G/9.98G [00:35<00:12, 145MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.15G/9.98G [00:35<00:09, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.18G/9.98G [00:35<00:08, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.21G/9.98G [00:35<00:07, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.24G/9.98G [00:35<00:06, 256MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.27G/9.98G [00:36<00:06, 268MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.30G/9.98G [00:36<00:06, 279MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.34G/9.98G [00:36<00:05, 285MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.37G/9.98G [00:36<00:05, 293MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.40G/9.98G [00:36<00:05, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.43G/9.98G [00:36<00:05, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.46G/9.98G [00:36<00:05, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.49G/9.98G [00:36<00:05, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.54G/9.98G [00:36<00:04, 302MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.57G/9.98G [00:37<00:04, 305MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.61G/9.98G [00:37<00:04, 310MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.64G/9.98G [00:37<00:04, 310MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.67G/9.98G [00:37<00:04, 310MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.70G/9.98G [00:37<00:04, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.73G/9.98G [00:37<00:04, 307MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.77G/9.98G [00:37<00:04, 299MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.80G/9.98G [00:37<00:03, 300MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.83G/9.98G [00:37<00:03, 302MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.86G/9.98G [00:38<00:03, 299MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.89G/9.98G [00:38<00:03, 300MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.92G/9.98G [00:38<00:03, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.95G/9.98G [00:38<00:03, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.99G/9.98G [00:38<00:03, 289MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.02G/9.98G [00:38<00:05, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.05G/9.98G [00:38<00:04, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.08G/9.98G [00:38<00:03, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.11G/9.98G [00:39<00:03, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.14G/9.98G [00:39<00:03, 254MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.18G/9.98G [00:39<00:03, 266MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.21G/9.98G [00:39<00:02, 269MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.24G/9.98G [00:39<00:02, 274MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.27G/9.98G [00:39<00:02, 283MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.98G [00:39<00:02, 289MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.33G/9.98G [00:39<00:02, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.37G/9.98G [00:39<00:02, 300MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.42G/9.98G [00:40<00:01, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.45G/9.98G [00:40<00:01, 306MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.48G/9.98G [00:40<00:01, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.51G/9.98G [00:40<00:01, 304MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.55G/9.98G [00:40<00:01, 309MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.59G/9.98G [00:40<00:01, 314MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.64G/9.98G [00:40<00:01, 316MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.68G/9.98G [00:40<00:00, 306MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.72G/9.98G [00:41<00:00, 306MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.75G/9.98G [00:41<00:00, 307MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.78G/9.98G [00:41<00:00, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.81G/9.98G [00:41<00:00, 309MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.86G/9.98G [00:41<00:00, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.89G/9.98G [00:41<00:00, 308MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.92G/9.98G [00:41<00:00, 303MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [00:41<00:00, 238MB/s]\n",
            "Downloading shards:  50% 1/2 [00:42<00:42, 42.10s/it]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 31.5M/3.50G [00:00<00:11, 305MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 62.9M/3.50G [00:00<00:12, 285MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 94.4M/3.50G [00:00<00:11, 289MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 126M/3.50G [00:00<00:11, 285MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 157M/3.50G [00:00<00:11, 282MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 189M/3.50G [00:00<00:11, 282MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 220M/3.50G [00:00<00:11, 282MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 252M/3.50G [00:00<00:11, 283MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 283M/3.50G [00:00<00:11, 288MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 315M/3.50G [00:01<00:11, 288MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 346M/3.50G [00:01<00:10, 288MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 377M/3.50G [00:01<00:10, 285MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 409M/3.50G [00:01<00:10, 283MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 440M/3.50G [00:01<00:10, 285MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 472M/3.50G [00:01<00:10, 286MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 503M/3.50G [00:01<00:10, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 535M/3.50G [00:01<00:10, 283MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 566M/3.50G [00:02<00:11, 256MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 598M/3.50G [00:02<00:11, 262MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 629M/3.50G [00:02<00:10, 272MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 661M/3.50G [00:02<00:10, 279MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 692M/3.50G [00:02<00:09, 284MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 724M/3.50G [00:02<00:09, 289MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 755M/3.50G [00:02<00:09, 290MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 786M/3.50G [00:02<00:09, 293MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 818M/3.50G [00:02<00:09, 297MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 860M/3.50G [00:03<00:08, 303MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 891M/3.50G [00:03<00:08, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 923M/3.50G [00:03<00:08, 303MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 954M/3.50G [00:03<00:08, 305MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 986M/3.50G [00:03<00:08, 303MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.02G/3.50G [00:03<00:08, 305MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.05G/3.50G [00:03<00:08, 301MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.08G/3.50G [00:03<00:08, 302MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.11G/3.50G [00:03<00:07, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.14G/3.50G [00:03<00:07, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.17G/3.50G [00:04<00:07, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.22G/3.50G [00:04<00:07, 308MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.26G/3.50G [00:04<00:07, 310MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.29G/3.50G [00:04<00:07, 311MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.33G/3.50G [00:04<00:06, 313MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.36G/3.50G [00:04<00:06, 312MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.41G/3.50G [00:04<00:06, 313MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.44G/3.50G [00:04<00:06, 311MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.47G/3.50G [00:04<00:06, 309MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.50G/3.50G [00:05<00:06, 309MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.53G/3.50G [00:05<00:06, 309MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.56G/3.50G [00:05<00:06, 306MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.59G/3.50G [00:05<00:06, 307MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.63G/3.50G [00:05<00:06, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.66G/3.50G [00:05<00:06, 301MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.69G/3.50G [00:05<00:06, 294MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.72G/3.50G [00:05<00:06, 293MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.75G/3.50G [00:05<00:05, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.78G/3.50G [00:06<00:05, 293MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.81G/3.50G [00:06<00:05, 292MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.85G/3.50G [00:06<00:05, 289MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.88G/3.50G [00:06<00:05, 285MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.91G/3.50G [00:06<00:05, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.94G/3.50G [00:06<00:05, 288MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.97G/3.50G [00:06<00:05, 289MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 2.00G/3.50G [00:06<00:05, 271MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.03G/3.50G [00:06<00:05, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.07G/3.50G [00:07<00:05, 261MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.10G/3.50G [00:07<00:05, 272MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.13G/3.50G [00:07<00:04, 281MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.16G/3.50G [00:07<00:04, 283MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.19G/3.50G [00:07<00:04, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.22G/3.50G [00:07<00:04, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.25G/3.50G [00:07<00:04, 294MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.29G/3.50G [00:07<00:04, 297MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.32G/3.50G [00:08<00:06, 193MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.35G/3.50G [00:08<00:05, 217MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.38G/3.50G [00:08<00:04, 234MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.41G/3.50G [00:08<00:04, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.44G/3.50G [00:08<00:03, 266MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.47G/3.50G [00:08<00:03, 274MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.51G/3.50G [00:08<00:03, 284MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.54G/3.50G [00:08<00:03, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.58G/3.50G [00:08<00:03, 299MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.61G/3.50G [00:09<00:02, 302MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.65G/3.50G [00:09<00:02, 306MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.69G/3.50G [00:09<00:02, 310MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.74G/3.50G [00:09<00:02, 313MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.78G/3.50G [00:09<00:02, 314MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.82G/3.50G [00:09<00:02, 310MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.85G/3.50G [00:09<00:02, 310MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.88G/3.50G [00:09<00:01, 309MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.92G/3.50G [00:10<00:01, 307MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.95G/3.50G [00:10<00:01, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.98G/3.50G [00:10<00:01, 306MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.01G/3.50G [00:10<00:01, 307MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.04G/3.50G [00:10<00:01, 306MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.07G/3.50G [00:10<00:01, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.10G/3.50G [00:10<00:01, 300MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.14G/3.50G [00:10<00:01, 299MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.17G/3.50G [00:10<00:01, 297MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.20G/3.50G [00:11<00:01, 292MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.23G/3.50G [00:11<00:00, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.26G/3.50G [00:11<00:00, 288MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.29G/3.50G [00:11<00:00, 291MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.32G/3.50G [00:11<00:00, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.36G/3.50G [00:11<00:00, 290MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.39G/3.50G [00:11<00:00, 292MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.42G/3.50G [00:11<00:00, 292MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.45G/3.50G [00:11<00:00, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.50G/3.50G [00:12<00:00, 290MB/s]\n",
            "Downloading shards: 100% 2/2 [00:54<00:00, 27.14s/it]\n",
            "Loading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  8.50it/s]\n",
            "generation_config.json: 100% 162/162 [00:00<00:00, 1.02MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100% 24/24 [08:51<00:00, 22.14s/it]\n",
            "Output to data/mt_bench/model_answer/Phi-3-mini-4k-instruct.jsonl\n",
            "tokenizer_config.json: 100% 3.17k/3.17k [00:00<00:00, 25.2MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 12.1MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 2.37MB/s]\n",
            "added_tokens.json: 100% 293/293 [00:00<00:00, 2.60MB/s]\n",
            "special_tokens_map.json: 100% 568/568 [00:00<00:00, 4.04MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 904/904 [00:00<00:00, 7.13MB/s]\n",
            "configuration_phi3.py: 100% 10.4k/10.4k [00:00<00:00, 45.7MB/s]\n",
            "modeling_phi3.py: 100% 73.8k/73.8k [00:00<00:00, 927kB/s]\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "model.safetensors.index.json: 100% 16.3k/16.3k [00:00<00:00, 63.7MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.97G [00:00<00:56, 88.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.97G [00:00<00:25, 191MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/4.97G [00:00<00:16, 301MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.97G [00:00<00:13, 364MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 199M/4.97G [00:00<00:12, 392MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.97G [00:00<00:11, 409MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.97G [00:00<00:11, 421MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.97G [00:00<00:10, 428MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/4.97G [00:01<00:10, 430MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.97G [00:01<00:10, 441MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.97G [00:01<00:09, 452MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.97G [00:01<00:09, 448MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.97G [00:01<00:09, 439MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 671M/4.97G [00:01<00:10, 426MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.97G [00:01<00:09, 427MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.97G [00:01<00:09, 424MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.97G [00:02<00:09, 428MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.97G [00:02<00:09, 432MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.97G [00:02<00:09, 421MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.97G [00:02<00:09, 419MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:02<00:09, 417MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:02<00:09, 425MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:02<00:08, 429MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.97G [00:02<00:08, 427MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:03<00:08, 428MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:03<00:08, 429MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.97G [00:03<00:08, 433MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.97G [00:03<00:08, 437MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.97G [00:03<00:07, 441MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/4.97G [00:03<00:07, 444MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.97G [00:03<00:07, 446MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.97G [00:03<00:07, 441MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.67G/4.97G [00:03<00:07, 443MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.97G [00:04<00:07, 442MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:04<00:07, 442MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.97G [00:04<00:07, 431MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.97G [00:04<00:07, 430MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.97G [00:04<00:07, 433MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.97G [00:04<00:06, 431MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.97G [00:04<00:06, 427MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.97G [00:04<00:06, 430MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.97G [00:05<00:06, 420MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.97G [00:05<00:06, 424MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.97G [00:05<00:06, 432MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.97G [00:05<00:06, 435MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.97G [00:05<00:06, 432MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:05<00:05, 430MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.97G [00:05<00:05, 432MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [00:05<00:05, 437MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.97G [00:06<00:05, 440MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.97G [00:06<00:05, 439MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.97G [00:06<00:05, 431MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.97G [00:06<00:05, 432MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.97G [00:06<00:05, 427MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.97G [00:06<00:04, 431MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.97G [00:06<00:04, 431MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.97G [00:06<00:04, 427MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:07<00:04, 426MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.97G [00:07<00:04, 425MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.97G [00:07<00:04, 425MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.97G [00:07<00:04, 417MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [00:07<00:04, 407MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.22G/4.97G [00:07<00:04, 406MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [00:07<00:04, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.97G [00:07<00:04, 391MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.97G [00:07<00:04, 371MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:08<00:04, 341MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.97G [00:08<00:05, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.46G/4.97G [00:08<00:06, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.97G [00:08<00:07, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.97G [00:08<00:06, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.97G [00:08<00:05, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.97G [00:09<00:05, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.97G [00:09<00:05, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.97G [00:09<00:04, 270MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [00:09<00:04, 270MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [00:09<00:04, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.97G [00:09<00:04, 262MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.97G [00:09<00:04, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [00:10<00:04, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [00:10<00:04, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.97G [00:10<00:04, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [00:10<00:06, 170MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.97G [00:10<00:04, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.97G [00:10<00:03, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [00:10<00:02, 309MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.97G [00:11<00:02, 343MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [00:11<00:02, 371MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/4.97G [00:11<00:01, 387MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.97G [00:11<00:01, 402MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/4.97G [00:11<00:01, 411MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.97G [00:11<00:01, 423MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.97G [00:11<00:01, 422MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.97G [00:11<00:01, 424MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.97G [00:12<00:00, 429MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.60G/4.97G [00:12<00:00, 422MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [00:12<00:00, 424MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.97G [00:12<00:00, 422MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.97G [00:12<00:00, 426MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.81G/4.97G [00:12<00:00, 429MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.97G [00:12<00:00, 420MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.97G [00:12<00:00, 411MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:13<00:00, 381MB/s]\n",
            "Downloading shards:  50% 1/2 [00:13<00:13, 13.28s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:00<00:06, 394MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 94.4M/2.67G [00:00<00:06, 414MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 147M/2.67G [00:00<00:06, 420MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 189M/2.67G [00:00<00:05, 417MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 231M/2.67G [00:00<00:05, 413MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 283M/2.67G [00:00<00:05, 422MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 336M/2.67G [00:00<00:05, 414MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 388M/2.67G [00:00<00:05, 421MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 440M/2.67G [00:01<00:05, 427MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 493M/2.67G [00:01<00:05, 430MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 545M/2.67G [00:01<00:04, 429MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 598M/2.67G [00:01<00:04, 432MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 650M/2.67G [00:01<00:04, 430MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 703M/2.67G [00:01<00:04, 429MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 755M/2.67G [00:01<00:04, 429MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 807M/2.67G [00:01<00:04, 428MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 860M/2.67G [00:02<00:04, 425MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 912M/2.67G [00:02<00:04, 425MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 965M/2.67G [00:02<00:03, 428MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.02G/2.67G [00:02<00:03, 430MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.07G/2.67G [00:02<00:03, 431MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.12G/2.67G [00:02<00:03, 434MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.17G/2.67G [00:02<00:03, 434MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.23G/2.67G [00:02<00:03, 434MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.28G/2.67G [00:02<00:03, 428MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.33G/2.67G [00:03<00:03, 422MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.38G/2.67G [00:03<00:03, 422MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.44G/2.67G [00:03<00:02, 425MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.49G/2.67G [00:03<00:02, 420MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.54G/2.67G [00:03<00:02, 425MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.59G/2.67G [00:03<00:02, 424MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.65G/2.67G [00:03<00:02, 427MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:03<00:02, 427MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.75G/2.67G [00:04<00:02, 428MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.80G/2.67G [00:04<00:02, 426MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.86G/2.67G [00:04<00:01, 425MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.91G/2.67G [00:04<00:01, 429MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.96G/2.67G [00:04<00:01, 427MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.01G/2.67G [00:04<00:01, 425MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.07G/2.67G [00:04<00:01, 433MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.12G/2.67G [00:04<00:01, 439MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.17G/2.67G [00:05<00:01, 444MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.22G/2.67G [00:05<00:00, 448MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.28G/2.67G [00:05<00:00, 451MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.33G/2.67G [00:05<00:00, 446MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.38G/2.67G [00:05<00:00, 448MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.43G/2.67G [00:07<00:03, 72.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.49G/2.67G [00:07<00:01, 97.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.54G/2.67G [00:07<00:01, 128MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.59G/2.67G [00:08<00:00, 163MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:08<00:00, 326MB/s]\n",
            "Downloading shards: 100% 2/2 [00:21<00:00, 10.89s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.18s/it]\n",
            "generation_config.json: 100% 172/172 [00:00<00:00, 1.64MB/s]\n",
            "  0% 0/24 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 24/24 [13:29<00:00, 33.74s/it]\n",
            "Output to data/mt_bench/model_answer/fastchat-t5-3b-v1.0.jsonl\n",
            "tokenizer_config.json: 100% 2.40k/2.40k [00:00<00:00, 18.6MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 1.74MB/s]\n",
            "added_tokens.json: 100% 150/150 [00:00<00:00, 715kB/s]\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 12.8MB/s]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 1.52k/1.52k [00:00<00:00, 12.4MB/s]\n",
            "pytorch_model.bin: 100% 6.71G/6.71G [00:18<00:00, 355MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "generation_config.json: 100% 142/142 [00:00<00:00, 936kB/s]\n",
            "100% 24/24 [08:15<00:00, 20.65s/it]\n",
            "Output to data/mt_bench/model_answer/chatglm-6b.jsonl\n",
            "tokenizer_config.json: 100% 441/441 [00:00<00:00, 2.61MB/s]\n",
            "tokenization_chatglm.py: 100% 17.0k/17.0k [00:00<00:00, 46.6MB/s]\n",
            "ice_text.model: 100% 2.71M/2.71M [00:00<00:00, 29.4MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Peer-review-in-LLMs/llm_judge/gen_model_answer.py\", line 331, in <module>\n",
            "    run_eval(\n",
            "  File \"/content/Peer-review-in-LLMs/llm_judge/gen_model_answer.py\", line 54, in run_eval\n",
            "    get_answers_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Peer-review-in-LLMs/llm_judge/gen_model_answer.py\", line 114, in get_model_answers\n",
            "    model, tokenizer = load_model(\n",
            "  File \"/content/Peer-review-in-LLMs/llm_judge/config/model_adapter.py\", line 320, in load_model\n",
            "    model, tokenizer = adapter.load_model(model_path, kwargs)\n",
            "  File \"/content/Peer-review-in-LLMs/llm_judge/config/model_adapter.py\", line 778, in load_model\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 865, in from_pretrained\n",
            "    return tokenizer_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2110, in from_pretrained\n",
            "    return cls._from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2336, in _from_pretrained\n",
            "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/8b7d33596d18c5e83e2da052d05ca4db02e60620/tokenization_chatglm.py\", line 196, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\", line 367, in __init__\n",
            "    self._add_tokens(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\", line 467, in _add_tokens\n",
            "    current_vocab = self.get_vocab().copy()\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/8b7d33596d18c5e83e2da052d05ca4db02e60620/tokenization_chatglm.py\", line 248, in get_vocab\n",
            "    vocab = {self._convert_id_to_token(i): i for i in range(self.vocab_size)}\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/8b7d33596d18c5e83e2da052d05ca4db02e60620/tokenization_chatglm.py\", line 244, in vocab_size\n",
            "    return self.sp_tokenizer.num_tokens\n",
            "AttributeError: 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'. Did you mean: '_tokenize'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt6qbvL6JP6z",
        "outputId": "afcbe183-1c6b-4493-88ff-9421bcdfa793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed.\n",
            "Transformation and Saving completed.\n",
            "Processing completed.\n",
            "Transformation and Saving completed.\n",
            "Processing completed.\n",
            "Transformation and Saving completed.\n"
          ]
        }
      ],
      "source": [
        "# After answers generated\n",
        "\n",
        "!python llm_judge/assign_judge.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv llm_judge/data/judge_prompts.jsonl data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGGV77Ad21Fl",
        "outputId": "f0800450-1929-405d-98ee-e8b17fdb4076"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assets\tcon_optimization  data\tllm_judge  main.py  README.md  requirements.txt\n",
            "judge_prompts.jsonl  mt_bench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvba0_ImJRmi",
        "outputId": "e5336d8b-8def-4fc2-a719-cf98c4b50e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 0 existing matches.\n",
            "Successfully skipped 0 existing matches.\n",
            "Loaded 0 existing matches.\n",
            "Successfully skipped 0 existing matches.\n",
            "Loaded 0 existing matches.\n",
            "Successfully skipped 0 existing matches.\n",
            "Loaded 0 existing matches.\n",
            "Successfully skipped 0 existing matches.\n",
            "Stats:\n",
            "{\n",
            "    \"bench_name\": \"mt_bench\",\n",
            "    \"mode\": \"pairwise-all\",\n",
            "    \"judge\": \"gpt-3.5-turbo\",\n",
            "    \"judge_model_path\": null,\n",
            "    \"baseline\": null,\n",
            "    \"model_list\": [\n",
            "        \"fastchat-t5-3b-v1.0\",\n",
            "        \"vicuna-7b-v1.5\",\n",
            "        \"gpt-3.5-turbo\"\n",
            "    ],\n",
            "    \"num_gpus\": 1,\n",
            "    \"max_gpu_memory\": \"35GB\",\n",
            "    \"batch_size\": 512,\n",
            "    \"total_num_questions\": 72,\n",
            "    \"total_num_matches\": 144,\n",
            "    \"output_path\": \"data/mt_bench/model_judgment/gpt-3.5-turbo_pair.jsonl\"\n",
            "}\n",
            "Press Enter to confirm...\n",
            "  0% 0/1 [00:00<?, ?it/s]Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "Error formatting multi-turn prompt: 'question'\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-3.5-turbo in organization org-MTtMRbFIbUoutjO74aMc4ajJ on tokens per min (TPM): Limit 60000, Used 59010, Requested 1055. Please try again in 65ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "<class 'openai.error.RateLimitError'> Rate limit reached for gpt-3.5-turbo in organization org-MTtMRbFIbUoutjO74aMc4ajJ on tokens per min (TPM): Limit 60000, Used 59116, Requested 1055. Please try again in 171ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
            "{'question_id': 1001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_1', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 1002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 1003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 2001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 2002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 2003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 3001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 3002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 3003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 4001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 4002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 4003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 5001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 5002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 5003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 6001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 6002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 6003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 7001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 7002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 7003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 8001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 8002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 8003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 9001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 9002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 9003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 10001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 10002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 10003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 11001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 11002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 11003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 12001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 12002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 12003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 13001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 13002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 13003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 14001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 14002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 14003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 15001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 15002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 15003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 16001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 16002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 16003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 17001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 17002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 17003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 18001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 18002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 18003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 19001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 19002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 19003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 20001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 20002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 20003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 21001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 21002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 21003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 22001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 22002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 22003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 23001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 23002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 23003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 24001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 24002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 24003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 1001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 1002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 1003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 2001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 2002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 2003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 3001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 3002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 3003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 4001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 4002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 4003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 5001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 5002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 5003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 6001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 6002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 6003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 7001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_1', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 7002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 7003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 8001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 8002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 8003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 9001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 9002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 9003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 10001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 10002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 10003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 11001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 11002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 11003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 12001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 12002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 12003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'tie', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 13001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 13002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 13003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 14001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 14002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 14003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 15001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 15002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 15003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 16001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 16002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 16003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 17001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 17002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 17003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 18001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 18002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 18003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 19001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 19002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 19003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 20001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 20002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 20003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 21001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 21002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 21003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 22001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 22002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 22003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 23001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_1', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 23002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 23003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 24001, 'model_1': 'vicuna-7b-v1.5', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 24002, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'gpt-3.5-turbo', 'g1_winner': 'model_2', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "{'question_id': 24003, 'model_1': 'fastchat-t5-3b-v1.0', 'model_2': 'vicuna-7b-v1.5', 'g1_winner': 'model_1', 'g2_winner': 'model_2', 'judge': ('gpt-3.5-turbo', 'pair-v2')} \n",
            "\n",
            "100% 1/1 [02:47<00:00, 167.06s/it]\n"
          ]
        }
      ],
      "source": [
        "# Judgment of Pairs; Specify (real, existing on HuggingFace) Judge Model Name or Names.\n",
        "# Error: judge model smth smth\n",
        "# Error: no judge_prompts file in Data\n",
        "# Error:\n",
        "\n",
        "!export OPENAI_API_KEY=''\n",
        "!python llm_judge/gen_judgment.py \\\n",
        "--mode pairwise-all \\\n",
        "--new-judge-model gpt-3.5-turbo \\\n",
        "--model-list fastchat-t5-3b-v1.0 vicuna-7b-v1.5 gpt-3.5-turbo \\\n",
        "--batch-size 512 \\\n",
        "--bench-name mt_bench\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-sM8rbxJT6E",
        "outputId": "c9227161-d574-4a87-8a97-a4b83bd97c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\tcon_optimization  data\tllm_judge  main.py  README.md  requirements.txt\n",
            "gpt-3.5-turbo_pair.jsonl  gpt-3.5-turbo_pair_short.jsonl\n",
            "0\n",
            "gpt-3.5-turbo\n",
            "guanaco-33b-merged\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Peer-review-in-LLMs/con_optimization/main_ablation.py\", line 356, in <module>\n",
            "    df = pd.read_json(f'../llm_judge/data/mt_bench/model_judgment/{model_str}_pair.jsonl', lines=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 784, in read_json\n",
            "    return json_reader.read()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 973, in read\n",
            "    obj = self._get_object_parser(self._combine_lines(data_lines))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 1001, in _get_object_parser\n",
            "    obj = FrameParser(json, **kwargs).parse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 1134, in parse\n",
            "    self._parse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 1320, in _parse\n",
            "    loads(json, precise_float=self.precise_float), dtype=None\n",
            "ValueError: Expected object or value\n"
          ]
        }
      ],
      "source": [
        "# Ablation\n",
        "!ls\n",
        "!ls data/mt_bench/model_judgment\n",
        "!cd con_optimization && python main_ablation.py --baseline 0 --mode Order --epoch 30"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/final_project_int.zip /content/Peer-review-in-LLMs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51uNZg7ATmyZ",
        "outputId": "94673e12-2374-4dd5-85bf-3b1eb6053876"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/Peer-review-in-LLMs/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/requirements.txt (deflated 27%)\n",
            "  adding: content/Peer-review-in-LLMs/README.md (deflated 61%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.4_Peer_review_seed4.log (deflated 62%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.7_Order_seed4.log (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.7_Uniform_seed4.log (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.1_Uniform_seed4.log (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.1_Order_seed4.log (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.4_Reversed_seed4.log (deflated 78%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.1_Peer_review_seed4.log (deflated 63%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.4_Order_seed4.log (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac1_Order_seed4.log (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac1_Peer_review_seed4.log (deflated 62%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac1_Reversed_seed4.log (deflated 78%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.7_Reversed_seed4.log (deflated 78%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.4_Uniform_seed4.log (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac1_Uniform_seed4.log (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.1_Reversed_seed4.log (deflated 78%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/log/frac0.7_Peer_review_seed4.log (deflated 62%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/main_ablation.py (deflated 78%)\n",
            "  adding: content/Peer-review-in-LLMs/con_optimization/utils.py (deflated 69%)\n",
            "  adding: content/Peer-review-in-LLMs/main.py (deflated 73%)\n",
            "  adding: content/Peer-review-in-LLMs/assets/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/assets/mt_bench.jpg (deflated 5%)\n",
            "  adding: content/Peer-review-in-LLMs/assets/Metric.png (deflated 4%)\n",
            "  adding: content/Peer-review-in-LLMs/assets/cute.png (deflated 2%)\n",
            "  adding: content/Peer-review-in-LLMs/assets/Alpaca.jpg (deflated 6%)\n",
            "  adding: content/Peer-review-in-LLMs/assets/law.jpg (deflated 6%)\n",
            "  adding: content/Peer-review-in-LLMs/assets/peer-review-cute.png (deflated 2%)\n",
            "  adding: content/Peer-review-in-LLMs/assets/chatbot.jpg (deflated 5%)\n",
            "  adding: content/Peer-review-in-LLMs/data/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/judge_prompts.jsonl (deflated 87%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_judgment/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_judgment/gpt-3.5-turbo_pair.jsonl (deflated 96%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_judgment/gpt-3.5-turbo_pair_short.jsonl (deflated 96%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/question.jsonl (deflated 69%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/fastchat-t5-3b-v1.0/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/fastchat-t5-3b-v1.0/vicuna-7b-v1.5.jsonl (deflated 86%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/fastchat-t5-3b-v1.0/fastchat-t5-3b-v1.0.jsonl (deflated 87%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/fastchat-t5-3b-v1.0/gpt-3.5-turbo.jsonl (deflated 85%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/fastchat-t5-3b-v1.0/question/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/fastchat-t5-3b-v1.0/question/question.jsonl (deflated 87%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/Phi-3-mini-4k-instruct.jsonl (deflated 73%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/vicuna-7b-v1.5.jsonl (deflated 74%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/vicuna-7b-v1.5/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/vicuna-7b-v1.5/vicuna-7b-v1.5.jsonl (deflated 86%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/vicuna-7b-v1.5/fastchat-t5-3b-v1.0.jsonl (deflated 87%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/vicuna-7b-v1.5/gpt-3.5-turbo.jsonl (deflated 85%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/vicuna-7b-v1.5/question/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/vicuna-7b-v1.5/question/question.jsonl (deflated 87%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/fastchat-t5-3b-v1.0.jsonl (deflated 75%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/gpt-3.5-turbo.jsonl (deflated 72%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/count.txt (deflated 69%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/gpt-3.5-turbo/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/gpt-3.5-turbo/vicuna-7b-v1.5.jsonl (deflated 86%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/gpt-3.5-turbo/fastchat-t5-3b-v1.0.jsonl (deflated 87%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/gpt-3.5-turbo/gpt-3.5-turbo.jsonl (deflated 85%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/gpt-3.5-turbo/question/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/data/mt_bench/model_answer/gpt-3.5-turbo/question/question.jsonl (deflated 87%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/config (deflated 30%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/packed-refs (deflated 27%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/refs/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/branches/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/description (deflated 14%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/info/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/info/exclude (deflated 28%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/HEAD (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/logs/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/logs/refs/heads/master (deflated 24%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/logs/refs/remotes/origin/HEAD (deflated 24%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/logs/HEAD (deflated 24%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/index (deflated 56%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/objects/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/objects/info/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/objects/pack/pack-defe4ca8fc1e026497e4ea72487e963ffee0d6cb.pack (deflated 1%)\n",
            "  adding: content/Peer-review-in-LLMs/.git/objects/pack/pack-defe4ca8fc1e026497e4ea72487e963ffee0d6cb.idx (deflated 23%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/model_registry.py (deflated 72%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/model_falcon.py (deflated 70%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/compression.py (deflated 75%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/apply_lora.py (deflated 59%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/upload_hub.py (deflated 60%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/make_delta.py (deflated 64%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/monkey_patch_non_inplace.py (deflated 71%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__init__.py (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/model_chatglm.py (deflated 62%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/xfastertransformer.py (deflated 59%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/gptq.py (deflated 61%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/model_adapter copy.py (deflated 84%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/llama_condense_monkey_patch.py (deflated 66%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/convert_fp16.py (deflated 56%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/utils.py (deflated 63%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/qwen_generation_utils.py (deflated 73%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/model_codet5p.py (deflated 65%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/exllama.py (deflated 67%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/model_xfastertransformer.py (deflated 65%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/rwkv_model.py (deflated 61%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/model_exllama.py (deflated 66%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/model_adapter.py (deflated 84%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/conversation.py (deflated 80%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/model_chatglm.cpython-310.pyc (deflated 37%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/utils.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/conversation.cpython-310.pyc (deflated 58%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/compression.cpython-310.pyc (deflated 44%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/model_falcon.cpython-310.pyc (deflated 37%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/xfastertransformer.cpython-310.pyc (deflated 39%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/model_exllama.cpython-310.pyc (deflated 35%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/gptq.cpython-310.pyc (deflated 38%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/model_codet5p.cpython-310.pyc (deflated 39%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/awq.cpython-310.pyc (deflated 40%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/model_xfastertransformer.cpython-310.pyc (deflated 36%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/monkey_patch_non_inplace.cpython-310.pyc (deflated 42%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/exllama.cpython-310.pyc (deflated 41%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/llama_condense_monkey_patch.cpython-310.pyc (deflated 43%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/model_adapter.cpython-310.pyc (deflated 68%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/__init__.cpython-310.pyc (deflated 23%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/__pycache__/constants.cpython-310.pyc (deflated 40%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/apply_delta.py (deflated 73%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/constants.py (deflated 50%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/config/awq.py (deflated 63%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/common.py (deflated 81%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/__init__.py (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/assign_judge.py (deflated 78%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/gen_judgment.py (deflated 80%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/gen_model_answer.py (deflated 75%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/__pycache__/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/__pycache__/common.cpython-310.pyc (deflated 52%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/data/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/data/mt_bench/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/data/mt_bench/model_judgment/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/data/mt_bench/model_judgment/gpt-3.5-turbo_pair.jsonl (deflated 95%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/data/mt_bench/model_judgment/gpt-3.5-turbo_pair_short.jsonl (deflated 96%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/data/mt_bench/question.jsonl (deflated 63%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/data/mt_bench/model_answer/ (stored 0%)\n",
            "  adding: content/Peer-review-in-LLMs/llm_judge/data/mt_bench/model_answer/gpt-3.5-turbo.jsonl (deflated 70%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/final_project.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RAlMYqI6WrFK",
        "outputId": "368a6072-35ca-490d-ec58-711dd644a886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aaca6bbe-fe57-4f22-96fd-9a84698d6961\", \"final_project.zip\", 48410781)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}